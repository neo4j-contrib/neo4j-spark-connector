
= Using Spark Structured Streaming API

Let's see how you can easily leverage the link:http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html[Spark Structured Streaming API] with the Neo4j Connector for Apache Spark.

Although the connector is the same, Spark streaming works very differently from Spark batching.
Here are some links to learn more about the Spark streaming approach:

* link:https://spark.apache.org/docs/latest/streaming-programming-guide.html#overview[Structured Streaming Programming Guide] from Spark website
* link:https://databricks.com/blog/2015/07/30/diving-into-apache-spark-streamings-execution-model.html[Diving into Apache Spark Streamingâ€™s Execution Model] by Databricks

== Neo4j Streaming Options

.List of available streaming options
|===
|Setting Name |Description |Default Value |Required

4+|*Sink*

|`checkpointLocation`
|Checkpoint file location (xref:#_checkpoint[see more])
|_(none)_
|Yes

4+|*Source*

|`streaming.property.name`
|The timestamp property used for batch reading. Read more xref:#_streaming_property_name[here].
|_(none)_
|Yes

|`streaming.from`
|This option is used to tell the connector from where to send data to the stream.
Read more xref:#_streaming_from[here].

**NOW**: start streaming from the moment the stream starts

**ALL**: send all the data in the DB to the stream before reading new data
|`NOW`
|Yes

|`streaming.query.offset`
|A valid Cypher READ_ONLY query that returns a long value.

(i.e. `MATCH (p:MyLabel) RETURN MAX(p.timestamp)`)

This is used to get the last timestamp in the database for a given query. More on this xref:#_notes_on_query_mode[here]
|_(none)_
|Yes, only for `query` mode

|===

== Sink

Writing a stream to Neo4j is pretty easy and can be done using any of the xref:writing.adoc#_write_data[3 write strategies].

[NOTE]
The same xref:quickstart.adoc#_schema[schema concepts] also apply here.
If you start a streaming read with an empty result set, you need to specify the schema using
the xref:quickstart.adoc#user-defined-schema[user defined schema], or the batch read will fail.

.Code example that reads from a Kafka topic and writes to Neo4j.
[source,python]
----
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder() \
    .master('local[*]') \
    .getOrCreate()

df = spark.readStream \
    .format("kafka") \
    .option("subscribe", "PeopleTopic") \
    .load()

query = df.writeStream \
    .format("org.neo4j.spark.DataSource") \
    .option("url", "bolt://localhost:7687") \
    .option("save.mode", "ErrorIfExists") \
    .option("checkpointLocation", "/tmp/checkpoint/myCheckPoint") \
    .option("labels", "Person") \
    .option("node.keys", "value") \
    .start()
----

As said, you can use any writing strategy: xref:writing.adoc#write-node[Labels], xref:writing.adoc#write-rel[Relationship], or xref:writing.adoc#write-query[Query].

The only difference is that you must set the `checkpointLocation` and `save.mode` options.

With `save.mode` you can control how the data are written. More info xref:writing.adoc#save-mode[here].

=== Checkpoint

The **checkpoint** is a file that allows Spark Structured Streaming to recover from failures.
Spark updates this file with the progress information and recover from that point in case of failure or query restart.
**This checkpoint location has to be a path in an HDFS compatible file system.**

Since the topic is wide and complex, we suggest you to read the link:https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#recovering-from-failures-with-checkpointing[official Spark documentation page].

== Source

Reading a stream from Neo4j requires some additional configuration.

Let's see the code first and then we can analyze all the options.


[source,python]
----
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder() \
    .master('local[*]') \
    .getOrCreate()

df = spark.readStream \
    .format("org.neo4j.spark.DataSource") \
    .option("url", "bolt://localhost:7687") \
    .option("labels", "Person") \
    .option("streaming.property.name", "timestamp") \
    .option("streaming.from", "NOW") \
    .load()
----

[source,python]
----
# Memory streaming format writes the streamed data to a SparkSQL table
# NOTE: make sure this code is executed in another block,
# or at least seconds later the previous one to allow the full initialization of the stream.
# The risk is that the query will return an empty result.
query = stream.writeStream \
    .format("memory") \
    .queryName("testReadStream") \
    .start()

spark \
  .sql("select * from testReadStream order by timestamp") \
  .show()
----

=== Streaming Property Name

For the streaming to work, we need each record to have a property of type `timestamp`
to leverage when reading new data from Neo4j to be sent to the stream.

Behind the scenes the connector is building a query with a where clause that checks for the
records that have`[timestampProperty] >= currentTimestamp() AND [timestampProperty] IS NOT NULL`.

So it's required that each node that has the timestamp property of a Neo4j type (datetime or timestamps),
and it *must* be not null.

[NOTE]
A property of type string like "2021-08-11" won't work. It needs to be a datetime or timestamp Neo4j type.

The property name can be anything, just remember to set the `streaming.property.name` accordingly.

=== Streaming From

You can decide to stream all the data in the db, or just the new ones.
To achieve this you can set the `streaming.from` option to one of these two values:

* `NOW`: will start reading from the current timestamp (This is the **default value for the `streaming.from` option**)
* `ALL`: will read all the data in the database first, and then just the new ones

=== Reading mode

As for Sink mode, you can use any of the reading strategies: link:reading.adoc#read-node[Labels], link:reading.adoc#read-rel[Relationship], or link:reading.adoc#read-query[Query].

==== Notes on `query` mode

Handling the `streaming.from` and `streaming.property.name` is a bit less automatic when using the query mode.

Let's see an example and then explain what's happening.

[source,python]
----
from pyspark.sql import SparkSession

spark = SparkSession \
    .builder() \
    .master('local[*]') \
    .getOrCreate()

df = spark.readStream \
    .format("org.neo4j.spark.DataSource") \
    .option("url", "bolt://localhost:7687") \
    .option("streaming.from", "NOW") \
    .option("streaming.property.name", "timestamp") \
    .option("query", \
        """MATCH (p:Test3_Person)
           WHERE p.timestamp > $stream.offset
           RETURN p.age AS age, p.timestamp AS timestamp""") \
    .option("streaming.query.offset", \
        "MATCH (p:Test3_Person) RETURN max(p.timestamp)") \
    .load()
----

As you can see, the `streaming.from` and `streaming.property.name` must be specified anyway, but you need to take care of the where clause yourself.
You are provided with a variable `$stream.offset` that contains the value of the timestamp that is being used.

In this case the `streaming.query.offset` option is mandatory;
this option is used by the connector to read the last timestamp in the database and the result will be set as value of the `$stream.offset` parameter.

=== Additional examples

You can find streaming code bits and many other examples on link:https://github.com/utnaf/neo4j-connector-apache-spark-notebooks[this repository] that contains Zeppelin notebooks.

A complete example using Spark, Neo4j and AWS Kinesis is described in the article link:link:https://medium.com/neo4j/from-kinesis-via-spark-to-neo4j-97d564562b61[From Kinesis via Spark to Neo4j].